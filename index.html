<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
    <script src="js/face-api.js"></script>
</head>
<body>
<div style="position:fixed; z-index: 10000;" id="holder">
    <button id="switch-source" onclick="switchsource(this)">switch webcam/video</button>
    <button id="switch-model" onclick="switchmodel(this)">switch model (now default)</button>
<br/>
    <canvas
            id="cropCvs"
            height="600"
            width="400"
            style="width: 400px; height: 600px; border: 1px solid black;">
    </canvas>


    <script>
        var current_source = 'video';
        var switching_lock = false;

        function switchsource(el) {
            if(switching_lock) return;
            function waiting(on){
                if(on){
                    switching_lock = true;
                    el.style.background = 'red';
                    el.diabled = true;
                }
                else{
                    setTimeout(function(){
                        switching_lock = false;
                        el.style.background = 'white';
                        el.disabled = false;
                    }, 500);

                }
            }
            waiting(true);
            if (current_source === 'video') {
                console.log('switching to webcam');
                navigator.mediaDevices.getUserMedia({video: {}}).then(s => {
                    faces.video.srcObject = s;
                    faces.video.onloadedmetadata = () => {
                        faces.video.play();
                    };
                }).catch(e => {
                    console.error(e);
                });
                current_source = 'webcam';

            }
            else if(current_source === 'webcam'){
                console.log('switching to video');
                faces.video.srcObject = undefined;
                faces.video.src = videofile;
                current_source = 'video';
            }
            waiting(false);
        }

        function switchmodel(el){
            if(!faces.model) faces.model = 'default'
            var new_model_i = faces.models.indexOf(faces.model) + 1;
            faces.model = faces.models[new_model_i % (faces.models.length)]
            el.innerText  = 'switch model (now '+faces.model+')';
        }


    </script>
</div>

<video id="video" onplay="faces.video_playing_now()" controls autoplay muted loop
       style="width: 100%!important;height: auto!important;">
    <source src="output.mp4" type="video/mp4">
</video>
<script>

    const faces = function() {
        var api = {};

        var models_loaded = false;
        var video_playing = false;
        var runloop = true;
        var canvas;
        var displaySize;
        const videofile = 'output.mp4';
        const video = document.getElementById('cropCvs');
        var model = 'default';
        var models = [model, 'mtcnnForwardParams', 'tinyfacedetector'];

        api.models = models;

        function video_playing_now() {
            video_playing = true;
        }
        api.video_playing_now = video_playing_now;
        api.video = video;

        document.addEventListener("DOMContentLoaded", function (event) {


            async function init() {
                const MODEL_URL = './facemodels';

                await faceapi.loadSsdMobilenetv1Model(MODEL_URL);
                await faceapi.loadFaceLandmarkModel(MODEL_URL);

                await faceapi.loadMtcnnModel(MODEL_URL);
                await faceapi.loadTinyFaceDetectorModel(MODEL_URL);
                await faceapi.loadFaceRecognitionModel(MODEL_URL);

            }

            //https://scholarworks.uark.edu/cgi/viewcontent.cgi?article=1073&context=csceuht
            const mtcnnForwardParams = {
                minFaceSize: 200,
                //scaleFactor: .5,
                maxNumScales: 20
            };

            async function run() {
                if (!models_loaded || !video_playing) return;

                displaySize = {width: video.offsetWidth, height: video.offsetHeight};
                if(video.tagName==='CANVAS'){
                    canvas =  document.createElement("CANVAS");
                }
                else {
                    canvas = faceapi.createCanvasFromMedia(video);
                }
                canvas.id = 'canvas';
                canvas.style.position = 'absolute';
                canvas.style.zIndex = '5000';
                canvas.style.left = '400px';
                canvas.style.top = '0';
                document.body.append(canvas);
                faceapi.matchDimensions(canvas, displaySize);

                startloop();
            }

            function startloop() {
                var interval = setInterval(async () => {
                    zoom()
                    if (!runloop) {
                        clearInterval(interval);
                    }

                    let detections;
                    if (model === 'default') {
                        const minProbability = 0.05;
                        detections = await faceapi.detectAllFaces(video, new faceapi.SsdMobilenetv1Options(), minProbability);
                    } else if (model === 'tinyfacedetector') {
                        const input_size = 416;
                        // https://github.com/justadudewhohacks/face-api.js/#tinyfacedetectoroptions
                        // size at which image is processed, the smaller the faster,
                        // but less precise in detecting smaller faces, must be divisible
                        // by 32, common sizes are 128, 160, 224, 320, 416, 512, 608,
                        // for face tracking via webcam I would recommend using smaller sizes,
                        // e.g. 128, 160, for detecting smaller faces use larger sizes, e.g. 512, 608
                        // default: 416
                        detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions(input_size))
                    } else if (model === 'mtcnnForwardParams') {
                        detections = await faceapi.mtcnn(video, mtcnnForwardParams)
                    } else {
                        throw('unknown model requested: ' + model)
                    }

                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                }, 100);
            }


            init().then(() => {
                models_loaded = true;
                run();
            });


        });
        return api;
    }();


</script>

<script>
    var _actual_video = document.getElementById('video')
    var canvas = document.getElementById('cropCvs');

    var canvas_height = 600;
    var canvas_width = 400;

    canvas.style.height =canvas.height = canvas_height.toString();
    canvas.style.width = canvas.width = canvas_width.toString();

    var ctx = canvas.getContext('2d');
    var offset_x = 2300;
    var offset_y = 150;
    function zoom() {
        ctx.drawImage(_actual_video, offset_x, offset_y, canvas_width / 2, canvas_height / 2, 0, 0, canvas_width, canvas_height);
    }

</script>
</body>
</html>